{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jailbreak Control Using PromptGuard\n",
    "\n",
    "LLM-powered applications are often targets of prompt attacks, which are crafted inputs designed to alter the intended behavior of the model. There are two main types of prompt attacks: **prompt injection** and **jailbreaking**.\n",
    "\n",
    "- **Prompt Injections** are inputs that exploit the inclusion of untrusted data from third parties or users within the model’s context window, leading to unintended instructions being executed.\n",
    "- **Jailbreaks** are malicious instructions aimed at bypassing the model’s built-in safety and security features.\n",
    "\n",
    "**PromptGuard** is an open-source, lightweight classifier developed by Meta and trained on an extensive corpus of attacks. It effectively detects both explicitly malicious prompts and prompts containing injected inputs. PromptGuard is available on Hugging Face: [PromptGuard 86M](https://huggingface.co/meta-llama/Prompt-Guard-86M).\n",
    "\n",
    "In addition, **LLamaGuard 3** extends the capabilities of its predecessor (LLama Guard 2) by adding detection for three new categories: Defamation, Elections, and Code Interpreter Abuse. This enhanced level of content safety is critical, as alignment alone cannot fully prevent unauthorized use of applications.\n",
    "\n",
    "## Hazard Categories\n",
    "\n",
    "LLamaGuard 3 covers the following hazard categories:\n",
    "\n",
    "| Code | Category                        |\n",
    "|------|---------------------------------|\n",
    "| S1   | Violent Crimes                  |\n",
    "| S2   | Non-Violent Crimes              |\n",
    "| S3   | Sex-Related Crimes              |\n",
    "| S4   | Child Sexual Exploitation       |\n",
    "| S5   | Defamation                      |\n",
    "| S6   | Specialized Advice              |\n",
    "| S7   | Privacy                         |\n",
    "| S8   | Intellectual Property           |\n",
    "| S9   | Indiscriminate Weapons          |\n",
    "| S10  | Hate                            |\n",
    "| S11  | Suicide & Self-Harm             |\n",
    "| S12  | Sexual Content                  |\n",
    "| S13  | Elections                       |\n",
    "| S14  | Code Interpreter Abuse          |\n",
    "\n",
    "This model is open-source and available on Hugging Face: [Llama-Guard 3 8B INT8](https://huggingface.co/meta-llama/Llama-Guard-3-8B-INT8).\n",
    "\n",
    "By combining PromptGuard and LLamaGuard 3, applications can implement a comprehensive security layer that guards against both direct prompt attacks (such as jailbreaks and prompt injections) and requests for unsafe content. Let’s explore how well PromptGuard performs in detecting jailbreak attempts.\n",
    "\n",
    "**Note**: You must be granted access to LLAMA before running this model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PromptGuard\n",
    "\n",
    "## Dataset\n",
    "\n",
    "To evaluate PromptGuard’s capabilities in detecting jailbreak attempts, we’ll use the **JailbreakHub dataset**. This dataset is specifically designed for testing jailbreak behaviors and is available on Hugging Face: [JailbreakHub](https://huggingface.co/datasets/walledai/JailbreakHub).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "jailbreak_dataset = load_dataset(\"walledai/JailbreakHub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'platform', 'source', 'jailbreak'],\n",
       "        num_rows: 15140\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "jailbreak_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferenc of PromtGuard\n",
    "\n",
    "The model is relatively compact, with only 86 million parameters, making it efficient enough to run smoothly even on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Pipeline, AutoModelForSequenceClassification\n",
    "from typing import Any\n",
    "from pydantic import PrivateAttr\n",
    "\n",
    "class PromptGuard():\n",
    "    def __init__(self, device:str=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Prompt-Guard-86M\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\"meta-llama/Prompt-Guard-86M\").to(self.device)\n",
    "\n",
    "    def get_class_probabilities(self, text, temperature=1.0):\n",
    "        \"\"\"\"\n",
    "       Evaluate the model on the given text with temperature-adjusted softmax.\n",
    "        Note, as this is a DeBERTa model, the input text should have a maximum length of 512.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text to classify.\n",
    "            temperature (float): The temperature for the softmax function. Default is 1.0.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: The probability of each class adjusted by the temperature.\n",
    "\n",
    "        \"\"\"    \n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = inputs.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "\n",
    "        scaled_logits = logits / temperature\n",
    "\n",
    "        probabilities = softmax(scaled_logits, dim=1)\n",
    "        return probabilities\n",
    "    \n",
    "    def get_jailbreak_score(self, text, temperature=1.0):\n",
    "        \"\"\"\n",
    "            Evaluate the probability that a given string contains malicious jailbreak or prompt injection.\n",
    "        Appropriate for filtering dialogue between a user and an LLM.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text to evaluate.\n",
    "            temperature (float): The temperature for the softmax function. Default is 1.0.\n",
    "            \n",
    "        Returns:\n",
    "            float: The probability of the text containing malicious content.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        probabilities = self.get_class_probabilities(text, temperature)\n",
    "        return probabilities[0, 2].item()\n",
    "    \n",
    "    def get_indirect_injection_score(self, text, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Evaluate the probability that a given string contains any embedded instructions (malicious or benign).\n",
    "        Appropriate for filtering third party inputs (e.g. web searches, tool outputs) into an LLM.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text to evaluate.\n",
    "            temperature (float): The temperature for the softmax function. Default is 1.0.\n",
    "            \n",
    "        Returns:\n",
    "            float: The combined probability of the text containing malicious or embedded instructions.\n",
    "        \"\"\"\n",
    "        probabilities = self.get_class_probabilities(text, temperature)\n",
    "        return (probabilities[0,1] + probabilities[0, 2]).item()\n",
    "    \n",
    "    def process_text_batch(self, texts, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Process a batch of texts and return their class probabilities.\n",
    "        Args:\n",
    "            model (transformers.PreTrainedModel): The loaded model.\n",
    "            tokenizer (transformers.PreTrainedTokenizer): The tokenizer for the model.\n",
    "            texts (list[str]): A list of texts to process.\n",
    "            temperature (float): The temperature for the softmax function.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: A tensor containing the class probabilities for each text in the batch.\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = inputs.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "        scaled_logits = logits / temperature\n",
    "        probabilities = softmax(scaled_logits, dim=-1)\n",
    "        return probabilities\n",
    "    \n",
    "    def get_scores_for_texts(self, texts, score_indices, temperature=1.0, max_batch_size=16):\n",
    "        \"\"\"\n",
    "        Compute scores for a list of texts, handling texts of arbitrary length by breaking them into chunks and processing in parallel.\n",
    "        Args:\n",
    "            model (transformers.PreTrainedModel): The loaded model.\n",
    "            tokenizer (transformers.PreTrainedTokenizer): The tokenizer for the model.\n",
    "            texts (list[str]): A list of texts to evaluate.\n",
    "            score_indices (list[int]): Indices of scores to sum for final score calculation.\n",
    "            temperature (float): The temperature for the softmax function.\n",
    "            max_batch_size (int): The maximum number of text chunks to process in a single batch.\n",
    "            \n",
    "        Returns:\n",
    "            list[float]: A list of scores for each text.\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        text_indices = []\n",
    "        for index, text in enumerate(texts):\n",
    "            chunks = [text[i:i+512] for i in range(0, len(text), 512)]\n",
    "            all_chunks.extend(chunks)\n",
    "            text_indices.extend([index] * len(chunks))\n",
    "        all_scores = [0] * len(texts)\n",
    "        for i in range(0, len(all_chunks), max_batch_size):\n",
    "            batch_chunks = all_chunks[i:i+max_batch_size]\n",
    "            batch_indices = text_indices[i:i+max_batch_size]\n",
    "            probabilities = self.process_text_batch(batch_chunks, temperature)\n",
    "            scores = probabilities[:, score_indices].sum(dim=1).tolist()\n",
    "            \n",
    "            for idx, score in zip(batch_indices, scores):\n",
    "                all_scores[idx] = max(all_scores[idx], score)\n",
    "        return all_scores\n",
    "    \n",
    "    def get_jailbreak_scores_for_texts(self, texts, temperature=1.0, max_batch_size=16):\n",
    "        \"\"\"\n",
    "        Compute jailbreak scores for a list of texts.\n",
    "        Args:\n",
    "            model (transformers.PreTrainedModel): The loaded model.\n",
    "            tokenizer (transformers.PreTrainedTokenizer): The tokenizer for the model.\n",
    "            texts (list[str]): A list of texts to evaluate.\n",
    "            temperature (float): The temperature for the softmax function.\n",
    "            max_batch_size (int): The maximum number of text chunks to process in a single batch.\n",
    "            \n",
    "        Returns:\n",
    "            list[float]: A list of jailbreak scores for each text.\n",
    "        \"\"\"\n",
    "        return self.get_scores_for_texts(texts, [2], temperature, max_batch_size)\n",
    "    \n",
    "    def get_indirect_injection_scores_for_texts(self, texts, temperature=1.0, max_batch_size=16):\n",
    "        \"\"\"\n",
    "        Compute indirect injection scores for a list of texts.\n",
    "        Args:\n",
    "            model (transformers.PreTrainedModel): The loaded model.\n",
    "            tokenizer (transformers.PreTrainedTokenizer): The tokenizer for the model.\n",
    "            texts (list[str]): A list of texts to evaluate.\n",
    "            temperature (float): The temperature for the softmax function.\n",
    "            max_batch_size (int): The maximum number of text chunks to process in a single batch.\n",
    "            \n",
    "        Returns:\n",
    "            list[float]: A list of indirect injection scores for each text.\n",
    "        \"\"\"\n",
    "        return self.get_scores_for_texts(texts, [1, 2], temperature, max_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/maruf/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import login\n",
    "HF_TOKEN = \"[Your_Token]\"\n",
    "\n",
    "login(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_guard = PromptGuard(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " A misty mountain landscape with towering pine trees, a calm river reflecting the scene ::20. A romantic painting by Caspar David Friedrich, adding ethereal light and mystery ::18. Sunrays filtering through dense fog, creating a surreal glow ::16. --v 4 --s 289 --style 4a --c 62 --ar 2:3\n",
      "\n",
      "This prompt captures a peaceful, mist-filled mountain scene, enhanced by Caspar David Friedrich's romantic influence, adding depth and serenity through ethereal lighting and detailed natural elements. The artistic parameters ensure a dreamlike, cohesive image that evokes tranquility\n",
      "\n",
      "[0.00015171305858530104]\n",
      "[0.9998831748962402]\n"
     ]
    }
   ],
   "source": [
    "example = \"\"\"\n",
    " A misty mountain landscape with towering pine trees, a calm river reflecting the scene ::20. A romantic painting by Caspar David Friedrich, adding ethereal light and mystery ::18. Sunrays filtering through dense fog, creating a surreal glow ::16. --v 4 --s 289 --style 4a --c 62 --ar 2:3\n",
    "\n",
    "This prompt captures a peaceful, mist-filled mountain scene, enhanced by Caspar David Friedrich's romantic influence, adding depth and serenity through ethereal lighting and detailed natural elements. The artistic parameters ensure a dreamlike, cohesive image that evokes tranquility\n",
    "\"\"\"\n",
    "print(example)\n",
    "print(prompt_guard.get_jailbreak_scores_for_texts([example]))\n",
    "print(prompt_guard.get_indirect_injection_scores_for_texts([example]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 0/474 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 474/474 [49:44<00:00,  6.30s/batch] \n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(jailbreak_dataset['train'], batch_size=batch_size, shuffle=False)\n",
    "with torch.no_grad():\n",
    "    for c in tqdm(dataloader, desc=\"Inference\", unit=\"batch\"):\n",
    "        inp = c['prompt']\n",
    "        true_label = c['jailbreak']\n",
    "        out = prompt_guard.get_jailbreak_scores_for_texts(inp)\n",
    "        v = [True if o>0.7 else False for o in out]\n",
    "        results.extend([(o,t.item()) for o,t in zip(v,true_label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5146631439894319\n",
      "Precision: 0.5627922457512268\n",
      "Recall: 0.6839489731275918\n",
      "F1Score: 0.4472876900028182\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_true = np.array([o[1] for o in results])\n",
    "y_pred = np.array([o[0] for o in results])\n",
    "print(\"Accuracy:\",accuracy_score(y_true, y_pred))\n",
    "t = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "print(f\"Precision: {t[0]}\")\n",
    "print(f\"Recall: {t[1]}\")\n",
    "print(f\"F1Score: {t[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6691"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len([r for r in y_pred if r==False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "No Jailbreak       0.98      0.48      0.64     13735\n",
      "   Jailbreak       0.15      0.89      0.25      1405\n",
      "\n",
      "    accuracy                           0.51     15140\n",
      "   macro avg       0.56      0.68      0.45     15140\n",
      "weighted avg       0.90      0.51      0.60     15140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "target_names = ['No Jailbreak', 'Jailbreak']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
