{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning TinyLLAMA on \"burkelibbey/colors\" Dataset\n",
    "\n",
    "This project fine-tunes the `TinyLLAMA` model using the `\"burkelibbey/colors\"` dataset to improve its ability to generate natural language descriptions for colors. Through this fine-tuning, TinyLLAMA becomes adept at interpreting and describing colors in response to prompts, making it useful for applications requiring accurate color-related text generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BS567\\Documents\\LLAMA\\LLAMA 3.2\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id=\"burkelibbey/colors\"\n",
    "model_id = \"C:/Users/BS567/Documents/LLM/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6\"\n",
    "output_model = \"tinyllama-colorist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_train(input, response)->str:\n",
    "    return f\"<|im_start|>user\\n{input}<|im_end|>\\n<|im_start|>assistant\\n{response}<|im_end|>\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_data(data_id):\n",
    "    data = load_dataset(data_id, split=\"train\")\n",
    "    print(data.column_names)\n",
    "    data_df = data.to_pandas()\n",
    "    data_df[\"text\"] = data_df[[\"description\", \"color\"]].apply(lambda x: \"<|im_start|>user\\n\" + x[\"description\"]+ \"<|im_end|>\\n<|im_start|>assistant\\n\" + x[\"color\"]+ \"<|im_end|>\\n\", axis=1)\n",
    "    data = Dataset.from_pandas(data_df)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['color', 'description']\n"
     ]
    }
   ],
   "source": [
    "data = prepare_train_data(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['color', 'description', 'text'],\n",
       "    num_rows: 33887\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'color': '#000000',\n",
       " 'description': 'Pure Black: A shade that completely absorbs light and does not reflect any colors. It is the darkest possible shade.',\n",
       " 'text': '<|im_start|>user\\nPure Black: A shade that completely absorbs light and does not reflect any colors. It is the darkest possible shade.<|im_end|>\\n<|im_start|>assistant\\n#000000<|im_end|>\\n'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bnb_configuration():\n",
    "   bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=\"float16\", bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "   return bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_tokenizer(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id);\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    bnb_config = get_bnb_configuration()\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config = bnb_config,\n",
    "        device_map = \"auto\"\n",
    "    )\n",
    "    model.config.use_cache=False\n",
    "    model.config_pretraining_tp=1\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = get_model_and_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CASUAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_model,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=2,\n",
    "    max_steps=100,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BS567\\Documents\\LLAMA\\LLAMA 3.2\\.venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\BS567\\Documents\\LLAMA\\LLAMA 3.2\\.venv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BS567\\Documents\\LLAMA\\LLAMA 3.2\\.venv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 33887/33887 [00:05<00:00, 6437.25 examples/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    args=training_arguments,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=False,\n",
    "    max_seq_length=1024\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]c:\\Users\\BS567\\Documents\\LLAMA\\LLAMA 3.2\\.venv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:655: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      " 10%|█         | 10/100 [17:17<2:42:42, 108.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4202, 'grad_norm': 2.4379231929779053, 'learning_rate': 0.00019510565162951537, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [34:38<2:19:47, 104.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7545, 'grad_norm': 2.2352042198181152, 'learning_rate': 0.00018090169943749476, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [52:17<2:04:42, 106.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3828, 'grad_norm': 0.9807547926902771, 'learning_rate': 0.00015877852522924732, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [1:10:08<1:50:23, 110.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1993, 'grad_norm': 0.7539818286895752, 'learning_rate': 0.00013090169943749476, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [1:28:24<1:30:32, 108.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0982, 'grad_norm': 0.7402814030647278, 'learning_rate': 0.0001, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [1:46:02<1:10:01, 105.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0609, 'grad_norm': 0.6680492162704468, 'learning_rate': 6.909830056250527e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [2:04:43<56:13, 112.43s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0109, 'grad_norm': 0.7770265340805054, 'learning_rate': 4.12214747707527e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [2:23:16<35:50, 107.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9964, 'grad_norm': 0.6150736808776855, 'learning_rate': 1.9098300562505266e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [2:40:42<17:15, 103.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9963, 'grad_norm': 0.5920224189758301, 'learning_rate': 4.8943483704846475e-06, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [2:58:33<00:00, 110.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9899, 'grad_norm': 0.6912044882774353, 'learning_rate': 0.0, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [2:58:34<00:00, 107.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 10714.9624, 'train_samples_per_second': 0.299, 'train_steps_per_second': 0.009, 'train_loss': 1.2909243869781495, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=1.2909243869781495, metrics={'train_runtime': 10714.9624, 'train_samples_per_second': 0.299, 'train_steps_per_second': 0.009, 'total_flos': 1941797864669184.0, 'train_loss': 1.2909243869781495, 'epoch': 0.09442870632672333})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load From Fine Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_prompt(question)-> str: \n",
    "    return f\"<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <|im_start|>user\n",
      "give me a sky blue "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color<|im_end|>\n",
      "<|im_start|>assistant: #119999<|im\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM, PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "model_path = \"tinyllama-colorist/checkpoint-100\"\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "prompt = formatted_prompt('give me a sky blue color')\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, eos_token_id=[tokenizer.eos_token_id],streamer=streamer, max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's, try measure the generation performance with regular inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "give me a red color<|im_end|>\n",
      "<|im_start|>assistant: #e01836<|im_end\n",
      "Time taken for inference: 51.28 seconds\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "from time import perf_counter\n",
    "\n",
    "prompt = formatted_prompt('give me a red color')\n",
    "\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "generation_config = GenerationConfig(penalty_alpha=0.6,do_sample = True,\n",
    "    top_k=5,temperature=0.5,repetition_penalty=1.2,\n",
    "    max_new_tokens=12,pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "start_time = perf_counter()\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "output_time = perf_counter() - start_time\n",
    "print(f\"Time taken for inference: {round(output_time,2)} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
