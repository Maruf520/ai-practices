{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Before we begin, please make sure to install the `torch` and `transformers` libraries, which are essential for working with language models in this notebook. Run the following commands:\n",
    "\n",
    "```bash\n",
    "# Install PyTorch (replace cu118 with your CUDA version, or use cpu-only if you don't have a GPU)\n",
    "pip install torch --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install Transformers\n",
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BS567\\Documents\\Purple LLAMA\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20b52a27a90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "torch.manual_seed(1230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_it(start,end):\n",
    "    nano = end-start\n",
    "    return nano/1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "max_token = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model in Full Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 4,400,196,480 bytes\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Print model size\n",
    "print(f\"Model size: {model.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John Smith and I am a software engineer. I have been working in the software industry for the past 5 years and have experience in developing web applications using various technologies such as Java, JavaScript, and HTML. I am proficient in using tools such as Git, JIRA, and Slack to manage projects and communicate with team members. I am also skilled in designing and implementing user-friendly interfaces using CSS and HTML. In my free time, I enjoy playing video games, reading books, and spending time with my family and friends. I am passionate about learning new technologies and staying up-to-date with the latest trends in the industry. I am looking forward to working with you and contributing to the development of the project. Thank you for considering my application. Best regards,\n",
      "\n",
      "[Your Name]\n",
      "Seconds: 144.3631253\n",
      "Initial GPU Memory Allocated: 4196.36 MB\n",
      "Final GPU Memory Allocated: 4204.49 MB\n",
      "Peak GPU Memory Used: 4215.21 MB\n",
      "Initial CPU Memory Usage: 2933.99 MB\n",
      "Final CPU Memory Usage: 2941.46 MB\n",
      "Memory increase in CPU: 7.47 MB\n",
      "Token/s: 1.2537827760646298\n"
     ]
    }
   ],
   "source": [
    "def time_it(start, end):\n",
    "    return (end - start) / 1e9 \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    initial_gpu_memory = torch.cuda.memory_allocated(device) / (1024 ** 2) \n",
    "else:\n",
    "    initial_gpu_memory = 0\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "initial_cpu_memory = process.memory_info().rss / (1024 ** 2) \n",
    "\n",
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "start = time.time_ns()\n",
    "outputs = model.generate(**inputs, max_new_tokens=max_token)\n",
    "end = time.time_ns()\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(output_text)\n",
    "\n",
    "t = time_it(start, end)\n",
    "print(\"Seconds:\", t)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    final_gpu_memory = torch.cuda.memory_allocated(device) / (1024 ** 2) \n",
    "    max_gpu_memory = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  \n",
    "else:\n",
    "    final_gpu_memory = max_gpu_memory = 0\n",
    "\n",
    "final_cpu_memory = process.memory_info().rss / (1024 ** 2) \n",
    "\n",
    "print(f\"Initial GPU Memory Allocated: {initial_gpu_memory:.2f} MB\")\n",
    "print(f\"Final GPU Memory Allocated: {final_gpu_memory:.2f} MB\")\n",
    "print(f\"Peak GPU Memory Used: {max_gpu_memory:.2f} MB\")\n",
    "print(f\"Initial CPU Memory Usage: {initial_cpu_memory:.2f} MB\")\n",
    "print(f\"Final CPU Memory Usage: {final_cpu_memory:.2f} MB\")\n",
    "print(f\"Memory increase in CPU: {final_cpu_memory - initial_cpu_memory:.2f} MB\")\n",
    "\n",
    "print(\"Token/s:\", len(outputs[0]) / t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INT4 and FP4 Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 746,773,376 bytes\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n",
    "print(f\"Model size: {model_4bit.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John and I am 25 years old. I am a student and I am studying in the University of London. I am a student of the University of London. I am a student of the University of London. I am a student of the University of London. I am a student of the University of London. I am a student of the University of London. I am a student of the University of London. I am a student of the University of London. I am a student of the University of London. I am a student of the University of London. I am a student of the University of London. I am a student of the University of London. I am a student of the University of London. I am a student of the University of London. I am a student of the University of London. I am a student of the University of London. I am a student of the University of London. I am a student of the University of London. I am a student of the\n",
      "Seconds: 14.6878761\n",
      "Initial GPU Memory Allocated: 786.01 MB\n",
      "Final GPU Memory Allocated: 786.02 MB\n",
      "Peak GPU Memory Used: 4215.21 MB\n",
      "Initial CPU Memory Usage: 943.10 MB\n",
      "Final CPU Memory Usage: 915.98 MB\n",
      "Memory increase in CPU: -27.12 MB\n",
      "Token/s: 13.9570894119947\n"
     ]
    }
   ],
   "source": [
    "def time_it(start, end):\n",
    "    return (end - start) / 1e9  \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    initial_gpu_memory = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
    "else:\n",
    "    initial_gpu_memory = 0\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "initial_cpu_memory = process.memory_info().rss / (1024 ** 2)\n",
    "\n",
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "start = time.time_ns()\n",
    "outputs = model_4bit.generate(**inputs, max_new_tokens=max_token)\n",
    "end = time.time_ns()\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(output_text)\n",
    "\n",
    "t = time_it(start, end)\n",
    "print(\"Seconds:\", t)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    final_gpu_memory = torch.cuda.memory_allocated(device) / (1024 ** 2) \n",
    "    max_gpu_memory = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  \n",
    "else:\n",
    "    final_gpu_memory = max_gpu_memory = 0\n",
    "\n",
    "final_cpu_memory = process.memory_info().rss / (1024 ** 2) \n",
    "\n",
    "print(f\"Initial GPU Memory Allocated: {initial_gpu_memory:.2f} MB\")\n",
    "print(f\"Final GPU Memory Allocated: {final_gpu_memory:.2f} MB\")\n",
    "print(f\"Peak GPU Memory Used: {max_gpu_memory:.2f} MB\")\n",
    "print(f\"Initial CPU Memory Usage: {initial_cpu_memory:.2f} MB\")\n",
    "print(f\"Final CPU Memory Usage: {final_cpu_memory:.2f} MB\")\n",
    "print(f\"Memory increase in CPU: {final_cpu_memory - initial_cpu_memory:.2f} MB\")\n",
    "\n",
    "print(\"Token/s:\", len(outputs[0]) / t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INT4 and NF4 Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 746,773,376 bytes\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n",
    "print(f\"Model size: {model_4bit.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BS567\\Documents\\Purple LLAMA\\.venv\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John Smith and I am a student at the University of XYZ. I am currently enrolled in the Bachelor of Science in Computer Science program. I am currently taking 12 credits and have completed 10 credits. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project\n",
      "Seconds: 14.6742134\n",
      "Initial GPU Memory Allocated: 1564.03 MB\n",
      "Final GPU Memory Allocated: 1564.03 MB\n",
      "Peak GPU Memory Used: 4215.21 MB\n",
      "Initial CPU Memory Usage: 829.70 MB\n",
      "Final CPU Memory Usage: 829.85 MB\n",
      "Memory increase in CPU: 0.15 MB\n",
      "Token/s: 13.970084420334246\n"
     ]
    }
   ],
   "source": [
    "def time_it(start, end):\n",
    "    return (end - start) / 1e9  \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    initial_gpu_memory = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
    "else:\n",
    "    initial_gpu_memory = 0\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "initial_cpu_memory = process.memory_info().rss / (1024 ** 2)  \n",
    "\n",
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "start = time.time_ns()\n",
    "outputs = model_4bit.generate(**inputs, max_new_tokens=max_token)\n",
    "end = time.time_ns()\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(output_text)\n",
    "\n",
    "t = time_it(start, end)\n",
    "print(\"Seconds:\", t)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    final_gpu_memory = torch.cuda.memory_allocated(device) / (1024 ** 2)  \n",
    "    max_gpu_memory = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  \n",
    "else:\n",
    "    final_gpu_memory = max_gpu_memory = 0\n",
    "\n",
    "final_cpu_memory = process.memory_info().rss / (1024 ** 2) \n",
    "\n",
    "\n",
    "print(f\"Initial GPU Memory Allocated: {initial_gpu_memory:.2f} MB\")\n",
    "print(f\"Final GPU Memory Allocated: {final_gpu_memory:.2f} MB\")\n",
    "print(f\"Peak GPU Memory Used: {max_gpu_memory:.2f} MB\")\n",
    "print(f\"Initial CPU Memory Usage: {initial_cpu_memory:.2f} MB\")\n",
    "print(f\"Final CPU Memory Usage: {final_cpu_memory:.2f} MB\")\n",
    "print(f\"Memory increase in CPU: {final_cpu_memory - initial_cpu_memory:.2f} MB\")\n",
    "\n",
    "print(\"Token/s:\", len(outputs[0]) / t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-Bit Nested Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 746,773,376 bytes\n"
     ]
    }
   ],
   "source": [
    "double_quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n",
    "print(f\"Model size: {model_4bit.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John Smith and I am a student at the University of XYZ. I am currently enrolled in the Bachelor of Science in Computer Science program. I am currently taking 12 credits and have completed 10 credits. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project\n",
      "Seconds: 23.1847848\n",
      "Initial GPU Memory Allocated: 1565.15 MB\n",
      "Final GPU Memory Allocated: 1565.15 MB\n",
      "Peak GPU Memory Used: 4215.21 MB\n",
      "Initial CPU Memory Usage: 1423.41 MB\n",
      "Final CPU Memory Usage: 1422.24 MB\n",
      "Memory increase in CPU: -1.17 MB\n",
      "Token/s: 8.842005727825432\n"
     ]
    }
   ],
   "source": [
    "def time_it(start, end):\n",
    "    return (end - start) / 1e9 \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    initial_gpu_memory = torch.cuda.memory_allocated(device) / (1024 ** 2) \n",
    "else:\n",
    "    initial_gpu_memory = 0\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "initial_cpu_memory = process.memory_info().rss / (1024 ** 2) \n",
    "\n",
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "start = time.time_ns()\n",
    "outputs = model_4bit.generate(**inputs, max_new_tokens=max_token)\n",
    "end = time.time_ns()\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(output_text)\n",
    "\n",
    "t = time_it(start, end)\n",
    "print(\"Seconds:\", t)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    final_gpu_memory = torch.cuda.memory_allocated(device) / (1024 ** 2)  \n",
    "    max_gpu_memory = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  \n",
    "else:\n",
    "    final_gpu_memory = max_gpu_memory = 0\n",
    "\n",
    "final_cpu_memory = process.memory_info().rss / (1024 ** 2)\n",
    "\n",
    "print(f\"Initial GPU Memory Allocated: {initial_gpu_memory:.2f} MB\")\n",
    "print(f\"Final GPU Memory Allocated: {final_gpu_memory:.2f} MB\")\n",
    "print(f\"Peak GPU Memory Used: {max_gpu_memory:.2f} MB\")\n",
    "print(f\"Initial CPU Memory Usage: {initial_cpu_memory:.2f} MB\")\n",
    "print(f\"Final CPU Memory Usage: {final_cpu_memory:.2f} MB\")\n",
    "print(f\"Memory increase in CPU: {final_cpu_memory - initial_cpu_memory:.2f} MB\")\n",
    "\n",
    "print(\"Token/s:\", len(outputs[0]) / t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_4bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Set of Quantization Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 746,773,376 bytes\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n",
    "print(f\"Model size: {model_4bit.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John Smith and I am a student at the University of XYZ. I am currently enrolled in the Bachelor of Science in Computer Science program. I am currently taking 12 credits and have completed 10 credits. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project\n",
      "Seconds: 17.0793422\n",
      "Initial GPU Memory Allocated: 786.52 MB\n",
      "Final GPU Memory Allocated: 786.52 MB\n",
      "Peak GPU Memory Used: 4215.21 MB\n",
      "Initial CPU Memory Usage: 1336.48 MB\n",
      "Final CPU Memory Usage: 1270.48 MB\n",
      "Memory increase in CPU: -66.00 MB\n",
      "Token/s: 12.002804182938615\n"
     ]
    }
   ],
   "source": [
    "def time_it(start, end):\n",
    "    return (end - start) / 1e9 \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    initial_gpu_memory = torch.cuda.memory_allocated(device) / (1024 ** 2) \n",
    "else:\n",
    "    initial_gpu_memory = 0\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "initial_cpu_memory = process.memory_info().rss / (1024 ** 2)  \n",
    "\n",
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "start = time.time_ns()\n",
    "outputs = model_4bit.generate(**inputs, max_new_tokens=max_token)\n",
    "end = time.time_ns()\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(output_text)\n",
    "\n",
    "t = time_it(start, end)\n",
    "print(\"Seconds:\", t)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    final_gpu_memory = torch.cuda.memory_allocated(device) / (1024 ** 2)  \n",
    "    max_gpu_memory = torch.cuda.max_memory_allocated(device) / (1024 ** 2) \n",
    "else:\n",
    "    final_gpu_memory = max_gpu_memory = 0\n",
    "\n",
    "final_cpu_memory = process.memory_info().rss / (1024 ** 2)\n",
    "\n",
    "print(f\"Initial GPU Memory Allocated: {initial_gpu_memory:.2f} MB\")\n",
    "print(f\"Final GPU Memory Allocated: {final_gpu_memory:.2f} MB\")\n",
    "print(f\"Peak GPU Memory Used: {max_gpu_memory:.2f} MB\")\n",
    "print(f\"Initial CPU Memory Usage: {initial_cpu_memory:.2f} MB\")\n",
    "print(f\"Final CPU Memory Usage: {final_cpu_memory:.2f} MB\")\n",
    "print(f\"Memory increase in CPU: {final_cpu_memory - initial_cpu_memory:.2f} MB\")\n",
    "\n",
    "print(\"Token/s:\", len(outputs[0]) / t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization Performance Summary\n",
    "| Quant | GPU Memory (MB)| CPU Memory (MB)  | Inference (Tokens/s) |\n",
    "| ------ | -------- | ------- | ------- |\n",
    "| Full Precision | 4204.49| 2941.46 | 1.25 |\n",
    "| 4 bit FP4 | 786.02 | 915.98 | 13.95 | \n",
    "| 4 bit Normal Float 4 | 1564.03 |829.85 | 13.97 |\n",
    "| Nested 4 bit | 1565.15 | 1422.24 | 8.84 |\n",
    "| All together | 786.52 | 1270.48 | 12.00 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
