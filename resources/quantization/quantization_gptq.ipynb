{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation Guide for Required Packages\n",
    "\n",
    "To run the script successfully, you need to install the following packages:\n",
    "\n",
    "- **auto-gptq**\n",
    "- **datasets**\n",
    "- **torch**\n",
    "- **transformers**\n",
    "\n",
    "You can install all of these packages with the following `pip` command:\n",
    "\n",
    "```bash\n",
    "pip install auto-gptq datasets torch transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BS567\\Documents\\Purple LLAMA\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "out_dir = model_id + \"-GPTQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,\n",
    "    group_size=128,\n",
    "    damp_percent=0.01,\n",
    "    desc_act=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BS567\\Documents\\Purple LLAMA\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaGPTQForCausalLM(\n",
       "  (model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(32000, 2048)\n",
       "      (layers): ModuleList(\n",
       "        (0-21): 22 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaSdpaAttention(\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config,torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BS567\\Documents\\Purple LLAMA\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\BS567\\.cache\\huggingface\\hub\\datasets--allenai--c4. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 356318 examples [00:06, 55948.62 examples/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (252167 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "data = load_dataset(\"allenai/c4\", data_files=\"en/c4-train.00001-of-01024.json.gz\", split=f\"train[:{n_samples*5}]\")\n",
    "tokenized_data = tokenizer(\"\\n\\n\".join(data['text']), return_tensors='pt')\n",
    "\n",
    "# Format tokenized examples\n",
    "examples_ids = []\n",
    "for _ in range(n_samples):\n",
    "    i = random.randint(0, tokenized_data.input_ids.shape[1] - tokenizer.model_max_length - 1)\n",
    "    j = i + tokenizer.model_max_length\n",
    "    input_ids = tokenized_data.input_ids[:, i:j]\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    examples_ids.append({'input_ids': input_ids, 'attention_mask': attention_mask})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - triton is not installed, reset use_triton to False\n",
      "INFO - Start quantizing layer 1/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 1/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 1/22...\n",
      "INFO - Start quantizing layer 2/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 2/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 2/22...\n",
      "INFO - Start quantizing layer 3/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 3/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 3/22...\n",
      "INFO - Start quantizing layer 4/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 4/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 4/22...\n",
      "INFO - Start quantizing layer 5/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 5/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 5/22...\n",
      "INFO - Start quantizing layer 6/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 6/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 6/22...\n",
      "INFO - Start quantizing layer 7/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 7/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 7/22...\n",
      "INFO - Start quantizing layer 8/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 8/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 8/22...\n",
      "INFO - Start quantizing layer 9/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 9/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 9/22...\n",
      "INFO - Start quantizing layer 10/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 10/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 10/22...\n",
      "INFO - Start quantizing layer 11/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 11/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 11/22...\n",
      "INFO - Start quantizing layer 12/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 12/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 12/22...\n",
      "INFO - Start quantizing layer 13/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 13/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 13/22...\n",
      "INFO - Start quantizing layer 14/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 14/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 14/22...\n",
      "INFO - Start quantizing layer 15/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 15/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 15/22...\n",
      "INFO - Start quantizing layer 16/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 16/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 16/22...\n",
      "INFO - Start quantizing layer 17/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 17/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 17/22...\n",
      "INFO - Start quantizing layer 18/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 18/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 18/22...\n",
      "INFO - Start quantizing layer 19/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 19/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 19/22...\n",
      "INFO - Start quantizing layer 20/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 20/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 20/22...\n",
      "INFO - Start quantizing layer 21/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 21/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 21/22...\n",
      "INFO - Start quantizing layer 22/22\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/22...\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/22...\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/22...\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/22...\n",
      "INFO - Quantizing mlp.up_proj in layer 22/22...\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/22...\n",
      "INFO - Quantizing mlp.down_proj in layer 22/22...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1h 20min 4s\n",
      "Wall time: 4h 8min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Quantize with GPTQ\n",
    "model.quantize(\n",
    "    examples_ids,\n",
    "    batch_size=1,\n",
    "    use_triton=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TinyLlama/TinyLlama-1.1B-Chat-v1.0-GPTQ\\\\tokenizer_config.json',\n",
       " 'TinyLlama/TinyLlama-1.1B-Chat-v1.0-GPTQ\\\\special_tokens_map.json',\n",
       " 'TinyLlama/TinyLlama-1.1B-Chat-v1.0-GPTQ\\\\tokenizer.model',\n",
       " 'TinyLlama/TinyLlama-1.1B-Chat-v1.0-GPTQ\\\\added_tokens.json',\n",
       " 'TinyLlama/TinyLlama-1.1B-Chat-v1.0-GPTQ\\\\tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "model.save_quantized(out_dir, use_safetensors=True)\n",
    "tokenizer.save_pretrained(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def time_it(start,end):\n",
    "    nano = end-start\n",
    "    return nano/1e9\n",
    "\n",
    "max_token = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Triton is not installed, reset use_triton to False.\n",
      "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
      "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
      "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
      "2. You are using pytorch without CUDA support.\n",
      "3. CUDA and nvcc are not installed in your device.\n",
      "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n",
      "INFO - The layer lm_head is not quantized.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Reload model and tokenizer\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    out_dir,\n",
    "    device=device,\n",
    "    use_triton=True,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John Smith and I am a software engineer. I have been working in the industry for the past 10 years. I have worked on various projects and have gained experience in various technologies. I am proficient in Java, Python, and C++ and have experience with various frameworks such as Django, Flask, and React. I have also worked on various web applications using technologies such as HTML, CSS, and JavaScript. I am a team player and have worked on various projects with\n",
      "Seconds: 46.6444157\n",
      "Token/s 2.2510733262331333\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "start = time.time_ns()\n",
    "outputs = model.generate(**inputs, max_new_tokens=max_token)\n",
    "end = time.time_ns()\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "t = time_it(start,end)\n",
    "print(\"Seconds:\",t)\n",
    "print(\"Token/s\",len(outputs[0])/t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 768,062,336 bytes\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model size: {model.get_memory_footprint():,} bytes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
