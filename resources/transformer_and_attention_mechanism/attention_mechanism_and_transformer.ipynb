{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Transformers and the Attention Mechanism\n",
    "\n",
    "Transformers have revolutionized natural language processing (NLP) and various fields in artificial intelligence. Introduced by Vaswani et al. in the landmark paper [\"Attention is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf) in 2017, the transformer architecture has since become the backbone of state-of-the-art models like BERT, GPT, and T5. Transformers are particularly effective in handling sequential data, such as text, by leveraging the **attention mechanism** to process data in parallel, making them faster and more efficient than traditional recurrent models like RNNs and LSTMs.\n",
    "\n",
    "## Understanding the Attention Mechanism\n",
    "\n",
    "The attention mechanism allows models to selectively focus on relevant parts of an input sequence when making predictions. Rather than processing each word or token in isolation or in strict sequential order, attention enables the model to dynamically weigh the importance of each token in relation to others. This is especially helpful in capturing long-range dependencies in sentences or sequences.\n",
    "\n",
    "### Key Components of Attention\n",
    "\n",
    "1. **Query, Key, and Value (Q, K, V)**: Each token in the sequence is transformed into three representations — the query, key, and value — through learned linear transformations. These representations enable the model to compute relevance scores (or attention weights) between tokens.\n",
    "2. **Scaled Dot-Product Attention**: The attention mechanism computes attention scores by taking the dot product between queries and keys. These scores are scaled and passed through a softmax function to generate probabilities, which are then used to weight the values, effectively focusing on relevant tokens.\n",
    "3. **Multi-Head Attention**: Multiple sets of attention heads allow the model to capture different aspects of relationships between tokens, enabling a richer representation.\n",
    "\n",
    "### Why Transformers Are Powerful\n",
    "\n",
    "The attention mechanism, combined with the ability to process tokens in parallel, makes transformers highly efficient for large datasets and tasks with long sequences. Unlike traditional RNNs, transformers don’t suffer from vanishing gradients over long sequences, making them more effective at capturing complex dependencies. This architecture has set the foundation for the development of models that excel in language understanding, generation, and various cross-domain applications.\n",
    "\n",
    "In summary, transformers and the attention mechanism together provide a robust framework for processing sequential data, transforming the field of NLP and paving the way for advancements in other AI domains.\n",
    "\n",
    "For more information, refer to the original paper: **[Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as f\n",
    "import torch\n",
    "from torch import nn\n",
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention Mechanism\n",
    "\n",
    "Transformers utilize a specialized attention mechanism known as **multi-head attention**, which enables the model to capture multiple types of relationships and dependencies within a sequence. Multi-head attention is fundamental to the power and flexibility of transformers, allowing them to process complex data in parallel and capture nuanced contextual information.\n",
    "\n",
    "Understanding transformers becomes straightforward once we grasp the concept of multi-head attention.\n",
    "\n",
    "Below is an illustration of attention and multi-head attention mechanisms, adapted from the original paper, [\"Attention is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf). \n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"../images/Attention.png\" alt=\"Diagram of Attention Mechanism\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Self-Attention\n",
    "\n",
    "We begin by understanding **scaled dot-product attention**, which is essential for building the multi-head attention layer in transformers. Mathematically, scaled dot-product attention is expressed as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "where:\n",
    "- **Q** (queries), **K** (keys), and **V** (values) are batches of matrices with shapes \\((\\text{batch\\_size}, \\text{seq\\_length}, \\text{num\\_features})\\).\n",
    "- **\\(d_k\\)** is the dimension of **Q** and **K**.\n",
    "- **\\(K^T\\)** refers to the transpose of **K**.\n",
    "\n",
    "Multiplying the query **Q** with the key **K** results in a matrix of shape \\((\\text{batch\\_size}, \\text{seq\\_length}, \\text{seq\\_length})\\). This matrix reveals the relevance of each element in the sequence, indicating the \"attention\" each element should receive relative to others.\n",
    "\n",
    "### Normalization with Softmax\n",
    "\n",
    "The resulting attention scores are then normalized using the softmax function, ensuring that all weights sum to one. This normalization highlights which elements in the sequence are more significant, guiding the model's focus.\n",
    "\n",
    "### Applying Attention to Values\n",
    "\n",
    "The final step involves applying the attention scores to the values **V** via matrix multiplication, determining the final weighted representation.\n",
    "\n",
    "> **Note**: For simplicity, we omit the optional masking operation shown in the original figure.\n",
    "\n",
    "\n",
    "In the following code, the matrix multiplications (MatMul) are implemented using `torch.bmm` in PyTorch. This is because **Q**, **K**, and **V** are batches of matrices with the shape \\((\\text{batch\\_size}, \\text{sequence\\_length}, \\text{num\\_features})\\), where batch matrix multiplication is performed over the last two dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scaled_dot_product_attention(query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "    temp = query.bmm(key.transpose(1, 2))\n",
    "    scale = query.size(-1) ** 0.5\n",
    "    softmax = f.softmax(temp / scale, dim=-1)\n",
    "    return softmax.bmm(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the diagram above, we see that multi-head attention is composed of several identical attention heads. Each attention head contains 3 linear layers, followed by scaled dot-product attention. Let's encapsulate this in an AttentionHead layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, dim_in: int, dim_k: int, dim_v: int):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(dim_in, dim_k)\n",
    "        self.k = nn.Linear(dim_in, dim_k)\n",
    "        self.v = nn.Linear(dim_in, dim_v)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "        return scaled_dot_product_attention(self.q(query), self.k(key), self.v(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Multi-Head Attention Layer\n",
    "\n",
    "The **multi-head attention layer** is an extension of the single-head attention mechanism, allowing the model to capture various relationships and dependencies across different parts of the input sequence. To create the multi-head attention layer, we simply combine multiple (i.e., `num_heads`) independent attention heads and add a linear layer for the output.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Each attention head in the multi-head attention layer:\n",
    "- Computes its own **query (Q)**, **key (K)**, and **value (V)** matrices.\n",
    "- Applies **scaled dot-product attention** independently.\n",
    "\n",
    "This means that each head can focus on a different part of the input sequence, attending to specific tokens or words independently of the others. By using multiple attention heads, the model can capture a broader range of relationships within the data, making it more robust and capable of handling complex sequences.\n",
    "\n",
    "Increasing the number of attention heads allows the model to \"pay attention\" to more parts of the sequence simultaneously, making it more powerful in capturing fine-grained information and enhancing its ability to learn contextual relationships.\n",
    "\n",
    "Overall, multi-head attention provides transformers with the flexibility to capture diverse and nuanced patterns, resulting in stronger and more versatile models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, dim_in: int, dim_k: int, dim_v: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(dim_in, dim_k, dim_v) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(num_heads * dim_v, dim_in)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "        return self.linear(\n",
    "            torch.cat([h(query, key, value) for h in self.heads], dim=-1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Positional Encoding\n",
    "\n",
    "To build the complete transformer, we need to introduce **positional encoding**. The multi-head attention mechanism itself does not have any trainable components that account for the order of tokens in a sequence. All operations are performed along the feature dimension, making it independent of sequence length and position. However, understanding the order of tokens is crucial for tasks where word order affects meaning.\n",
    "\n",
    "### Why Positional Encoding?\n",
    "\n",
    "Since the attention mechanism doesn’t inherently encode position information, we need a way to tell the model about the relative position of tokens within each input sequence. Vaswani et al. addressed this by using **positional encodings** generated from trigonometric functions, allowing the model to differentiate between positions without any explicit training.\n",
    "\n",
    "The encoding is defined by:\n",
    "\n",
    "$$\n",
    "\\text{PE}_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) \\quad \\text{and} \\quad \\text{PE}_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "where:\n",
    "- **pos** is the position of the token in the sequence.\n",
    "- **2i** and **2i+1** represent even and odd dimensions, respectively.\n",
    "- **d_model** is the model’s feature dimension (the total number of features per token).\n",
    "\n",
    "\n",
    "### Why Use Sinusoidal Functions?\n",
    "\n",
    "The authors experimented with learned embeddings for position encoding, but found that both methods yielded nearly identical results. They ultimately chose sinusoidal encoding due to its periodic nature, which allows the model to extrapolate to sequence lengths beyond those seen during training. Because sine and cosine functions are periodic and bounded within a [0, 1] range, they provide a consistent encoding pattern across sequences of varying lengths.\n",
    "\n",
    "This periodicity is beneficial during inference when processing sequences longer than those encountered during training. Sinusoidal encodings ensure that the model can generalize its understanding of position, even for unfamiliar lengths.\n",
    "\n",
    "While learned embeddings might be easier to implement and debug, sinusoidal encodings offer a theoretical advantage for handling longer sequences. In this explanation, we follow the authors’ approach and use sinusoidal encoding for positional information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding(seq_len: int, dim_model: int, device: torch.device = torch.device(\"cpu\"),) -> Tensor:\n",
    "    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
    "    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
    "    phase = (pos / 1e4) ** (dim // dim_model)\n",
    "\n",
    "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Transformer Architecture\n",
    "\n",
    "With all the core components in place, we can now assemble the **Transformer** model! Below is a diagram of the complete network architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"../images/Transformer.png\" alt=\"Transformer Architechture\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the **Transformer** model follows an encoder-decoder architecture. The **encoder** (left side of the diagram) processes the input sequence and generates a feature vector, also known as a **memory vector**. This memory vector is then passed to the **decoder** (right side), which processes the target sequence while incorporating information from the encoder’s memory. The output from the decoder represents the model’s final prediction.\n",
    "\n",
    "### Building the Encoder and Decoder Modules\n",
    "\n",
    "We can implement the encoder and decoder modules as separate components, combining them later to create the complete transformer model. Before diving into the implementation, there are a few more details to consider, particularly for the **feed-forward networks** within each layer of the encoder and decoder.\n",
    "\n",
    "### Feed-Forward Network Design\n",
    "\n",
    "Each layer in the encoder and decoder contains a **fully connected feed-forward network**. This network consists of two linear transformations with a ReLU activation function in between:\n",
    "- **Input and Output Dimensionality**: 512\n",
    "- **Inner Layer Dimensionality**: 2048\n",
    "\n",
    "This gives a simple implementation for the Feed Forward modules above:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(dim_input: int = 512, dim_feedforward: int = 2048) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim_input, dim_feedforward),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(dim_feedforward, dim_input),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Regularization\n",
    "\n",
    "What type of normalization should be applied? Is regularization necessary, such as dropout layers?\n",
    "\n",
    "In the Transformer model, the output of each sub-layer is calculated as **LayerNorm(x + Sublayer(x))**, where **Sublayer(x)** represents the function executed by the sub-layer itself. Dropout is applied to the output of each sub-layer before it is added to the original input and normalized. This approach enhances model generalization and stability by reducing overfitting.\n",
    "\n",
    "\n",
    "We can encapsulate all of this in a Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, *tensors: Tensor) -> Tensor:\n",
    "        # Assume that the \"value\" tensor is given last, so we can compute the\n",
    "        # residual.  This matches the signature of 'MultiHeadAttention'.\n",
    "        return self.norm(tensors[-1] + self.dropout(self.sublayer(*tensors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Encoder\n",
    "\n",
    "Now, let’s dive into building the encoder. With the utility methods we’ve set up, constructing the encoder is straightforward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim_model: int = 512, \n",
    "        num_heads: int = 6, \n",
    "        dim_feedforward: int = 2048, \n",
    "        dropout: float = 0.1, \n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_k = dim_v = dim_model // num_heads\n",
    "        self.attention = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_k, dim_v),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.feed_forward = Residual(\n",
    "            feed_forward(dim_model, dim_feedforward),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        src = self.attention(src, src, src)\n",
    "        return self.feed_forward(src)\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_layers: int = 6,\n",
    "        dim_model: int = 512, \n",
    "        num_heads: int = 8, \n",
    "        dim_feedforward: int = 2048, \n",
    "        dropout: float = 0.1, \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        seq_len, dimension = src.size(1), src.size(2)\n",
    "        src += position_encoding(seq_len, dimension)\n",
    "        for layer in self.layers:\n",
    "            src = layer(src)\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Decoder\n",
    "\n",
    "The decoder module is quite similar to the encoder, with a few key differences:\n",
    "\n",
    "- The decoder takes **two inputs**: the target sequence and the memory vector from the encoder.\n",
    "- Each layer in the decoder contains **two multi-head attention modules** instead of one.\n",
    "- The second multi-head attention module receives the memory vector as part of its input, allowing the decoder to incorporate information from the encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim_model: int = 512, \n",
    "        num_heads: int = 6, \n",
    "        dim_feedforward: int = 2048, \n",
    "        dropout: float = 0.1, \n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_k = dim_v = dim_model // num_heads\n",
    "        self.attention_1 = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_k, dim_v),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.attention_2 = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_k, dim_v),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.feed_forward = Residual(\n",
    "            feed_forward(dim_model, dim_feedforward),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
    "        tgt = self.attention_1(tgt, tgt, tgt)\n",
    "        tgt = self.attention_2(memory, memory, tgt)\n",
    "        return self.feed_forward(tgt)\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_layers: int = 6,\n",
    "        dim_model: int = 512, \n",
    "        num_heads: int = 8, \n",
    "        dim_feedforward: int = 2048, \n",
    "        dropout: float = 0.1, \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.linear = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
    "        seq_len, dimension = tgt.size(1), tgt.size(2)\n",
    "        tgt += position_encoding(seq_len, dimension)\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory)\n",
    "\n",
    "        return torch.softmax(self.linear(tgt), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Transformer Class\n",
    "\n",
    "Finally, we’ll combine everything into a single **Transformer class**. This step is straightforward: we simply bring together the encoder and decoder modules and ensure data flows through them in the proper sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_encoder_layers: int = 6,\n",
    "        num_decoder_layers: int = 6,\n",
    "        dim_model: int = 512, \n",
    "        num_heads: int = 6, \n",
    "        dim_feedforward: int = 2048, \n",
    "        dropout: float = 0.1, \n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(\n",
    "            num_layers=num_encoder_layers,\n",
    "            dim_model=dim_model,\n",
    "            num_heads=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.decoder = TransformerDecoder(\n",
    "            num_layers=num_decoder_layers,\n",
    "            dim_model=dim_model,\n",
    "            num_heads=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor) -> Tensor:\n",
    "        return self.decoder(tgt, self.encoder(src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Testing the Model\n",
    "\n",
    "With everything in place, it’s time to test the model to ensure our implementation works correctly. We’ll create random tensors for `src` and `tgt`, run them through the model to verify it executes without errors, and confirm that the output tensor has the expected shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 512])\n"
     ]
    }
   ],
   "source": [
    "src = torch.rand(64, 16, 512)\n",
    "tgt = torch.rand(64, 16, 512)\n",
    "out = Transformer()(src, tgt)\n",
    "print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
